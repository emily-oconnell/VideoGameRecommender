{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in /opt/conda/lib/python3.8/site-packages (7.5.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /opt/conda/lib/python3.8/site-packages (from praw) (1.2.3)\n",
      "Requirement already satisfied: update-checker>=0.18 in /opt/conda/lib/python3.8/site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in /opt/conda/lib/python3.8/site-packages (from praw) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.3.0 in /opt/conda/lib/python3.8/site-packages (from update-checker>=0.18->praw) (2.24.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.3.0->update-checker>=0.18->praw) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.3.0->update-checker>=0.18->praw) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.3.0->update-checker>=0.18->praw) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.3.0->update-checker>=0.18->praw) (2020.6.20)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.8/site-packages (3.6.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from nltk) (4.48.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.8/site-packages (from nltk) (2022.1.18)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.2 MB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.8/site-packages (from spacy) (4.48.2)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp38-cp38-manylinux2014_x86_64.whl (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 71 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.8/site-packages (from spacy) (2.24.0)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 13.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 6.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc<8.1.0,>=8.0.12\n",
      "  Downloading thinc-8.0.13-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (636 kB)\n",
      "\u001b[K     |████████████████████████████████| 636 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from spacy) (49.6.0.post20200814)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.9 MB 6.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from spacy) (20.4)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from spacy) (2.11.2)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
      "\u001b[K     |████████████████████████████████| 454 kB 5.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.9.0-py3-none-any.whl (25 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.6-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.8/site-packages (from spacy) (1.19.1)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.0.1-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Collecting smart-open<6.0.0,>=5.0.0\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 5.5 MB/s  eta 0:00:01     |██████████████████████▍         | 40 kB 13.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->spacy) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Installing collected packages: typing-extensions, pydantic, typer, smart-open, pathy, murmurhash, cymem, preshed, wasabi, spacy-loggers, langcodes, blis, catalogue, srsly, thinc, spacy-legacy, spacy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.7.4.2\n",
      "    Uninstalling typing-extensions-3.7.4.2:\n",
      "      Successfully uninstalled typing-extensions-3.7.4.2\n",
      "Successfully installed blis-0.7.5 catalogue-2.0.6 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.6 pathy-0.6.1 preshed-3.0.6 pydantic-1.8.2 smart-open-5.2.1 spacy-3.2.1 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 thinc-8.0.13 typer-0.4.0 typing-extensions-4.0.1 wasabi-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access the reddit api\n",
    "%pip install praw\n",
    "%pip install nltk\n",
    "%pip install spacy\n",
    "\n",
    "import pandas as pd\n",
    "import praw\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"max_rows\", None)\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import csv\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sign in to our reddit account\n",
    "reddit = praw.Reddit(client_id='GsD6yO1LgjoIQOYfyPq7Kg',\n",
    "                     client_secret='UDdjKE1yalj0pE7dEZndqhNgCW1pTQ', user_agent='CapstoneVG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.9 MB 14.6 MB/s eta 0:00:01   |███████████████▌                | 6.7 MB 2.2 MB/s eta 0:00:04     |████████████████▌               | 7.1 MB 2.2 MB/s eta 0:00:04\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /opt/conda/lib/python3.8/site-packages (from en-core-web-sm==3.2.0) (3.2.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.13)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.24.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (20.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.48.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.19.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (49.6.0.post20200814)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.8/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.0.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.2.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7f6d8471a4f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_list = []\n",
    "id_list = []\n",
    "comment_list = []\n",
    "c_dict = {}\n",
    "\n",
    "\n",
    "# get titles and ids\n",
    "hot_posts = reddit.subreddit('gamingsuggestions').hot(limit=2)\n",
    "for post in hot_posts:\n",
    "    title_list.append(post.title)\n",
    "    id_list.append(post.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sea9se</td>\n",
       "      <td>New rule regarding NFT/Pay 2 Earn/Blockchain g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sg80tr</td>\n",
       "      <td>Games where you unravel a conspiracy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              Title\n",
       "0  sea9se  New rule regarding NFT/Pay 2 Earn/Blockchain g...\n",
       "1  sg80tr               Games where you unravel a conspiracy"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initalize df for titles of posts\n",
    "df = pd.DataFrame({ 'id': id_list, 'Title': title_list})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all the comments of a post\n",
    "for p_id in df['id']:\n",
    "    submission = reddit.submission(id=p_id)\n",
    "    # get each comment for a specific post\n",
    "    for top_level_comment in submission.comments:\n",
    "        # only good comments plz\n",
    "        if(top_level_comment.score) >=10:\n",
    "            comment_list.append(top_level_comment.body)\n",
    "    c_dict[p_id] = comment_list\n",
    "    comment_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deus Ex (2000)',\n",
       " 'Return of the Obra Dinn',\n",
       " 'Zero Escape: 999 and Danganronpa, though they are anime games unlike what you mentioned. Also, the Portal games.',\n",
       " 'Outer Wilds, The Witness',\n",
       " 'Outer Wilds is exactly this. The entire game /u/myopicsurgeon',\n",
       " 'Judgement and lost judgement',\n",
       " 'Paradise Killer, great vaporwave-ish aesthetic and awesome character designs and soundtrack.',\n",
       " 'Horizon Zero Dawn',\n",
       " \"I'm surprised Control hasn't been mentioned yet. You unravel  the mystery of a strange shifting building that acts a bit like a SCP facility.\",\n",
       " 'Disco Elysium',\n",
       " 'Dishonored, maybe.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# id a nd list of comments\n",
    "df2 = pd.DataFrame(c_dict.keys(), columns= ['id'])\n",
    "df2['comments']= c_dict.values()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Deus Ex (2000)Return of the Obra DinnZero Escape: 999 and Danganronpa, though they are anime games unlike what you mentioned. Also, the Portal games.Outer Wilds, The WitnessOuter Wilds is exactly this. The entire game /u/myopicsurgeonJudgement and lost judgementParadise Killer, great vaporwave-ish aesthetic and awesome character designs and soundtrack.Horizon Zero DawnI'm surprised Control hasn't been mentioned yet. You unravel  the mystery of a strange shifting building that acts a bit like a SCP facility.Disco ElysiumDishonored, maybe.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_string = \"\".join(df2['comments'][1])\n",
    "comment_string\n",
    "#word_tokenize(comment_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Deus Ex', '(2000)Return', 'the Obra DinnZero Escape', 'Danganronpa', 'they', 'anime games', 'what', 'you', 'Also, the Portal games', 'Outer Wilds', 'The WitnessOuter Wilds', 'exactly this', 'The entire game', '/u/myopicsurgeonJudgement', 'judgementParadise Killer', 'great vaporwave-ish aesthetic and awesome character designs', 'soundtrack', \"Horizon Zero DawnI'm\", 'Control', 'You', 'the mystery', 'a strange shifting building', 'that', 'a SCP facility', 'Disco ElysiumDishonored']\n",
      "Verbs: ['mention', 'lose', 'surprise', 'mention', 'unravel', 'act']\n",
      "the Obra DinnZero Escape WORK_OF_ART\n",
      "999 CARDINAL\n",
      "Danganronpa GPE\n",
      "Portal ORG\n",
      "Outer Wilds PERSON\n",
      "The WitnessOuter Wilds ORG\n",
      "judgementParadise Killer PERSON\n",
      "Zero CARDINAL\n",
      "DawnI'm ORG\n",
      "Control ORG\n",
      "SCP ORG\n",
      "Disco ElysiumDishonored PERSON\n"
     ]
    }
   ],
   "source": [
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp(comment_string)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "title_comments = pd.merge(df, df2, on='id', how='left')\n",
    "comment_list = title_comments['comments'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(comment_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dubious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Deus/NNP Ex/NNP)\n",
      "  (/(\n",
      "  2000/CD\n",
      "  )/)\n",
      "  (PERSON Outer/NNP Wilds/NNP)\n",
      "  ,/,\n",
      "  The/DT\n",
      "  (ORGANIZATION WitnessZero/NNP)\n",
      "  Escape/NNP\n",
      "  :/:\n",
      "  999/CD\n",
      "  and/CC\n",
      "  (GPE Danganronpa/NNP)\n",
      "  ,/,\n",
      "  though/IN\n",
      "  they/PRP\n",
      "  are/VBP\n",
      "  anime/JJ\n",
      "  games/NNS\n",
      "  unlike/IN\n",
      "  what/WP\n",
      "  you/PRP\n",
      "  mentioned/VBD\n",
      "  ./.\n",
      "  Also/RB\n",
      "  ,/,\n",
      "  the/DT\n",
      "  (GPE Portal/NNP)\n",
      "  games.Return/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION Obra/NNP)\n",
      "  DinnHorizon/NNP\n",
      "  Zero/NNP\n",
      "  DawnDisco/NNP\n",
      "  Elysium/NNP)\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import ne_chunk\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "sent = preprocess(comment_string)\n",
    "\n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "#print(cs)\n",
    "\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "#pprint(iob_tagged)\n",
    "\n",
    "ne_tree = ne_chunk(pos_tag(word_tokenize(comment_string)))\n",
    "print(ne_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import csv\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "\n",
    "from nltk.corpus import conll2000\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk import pos_tag\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3> Anything after this is questionable, but maybe useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = []\n",
    "gaming_subreddit = reddit.subreddit('gamingsuggestions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in gaming_subreddit.hot(limit=10):\n",
    "    posts.append([post.title, post.score, post.id, post.subreddit,\n",
    "                 post.url, post.num_comments, post.selftext, post.created])\n",
    "posts = pd.DataFrame(posts, columns=[\n",
    "                     'title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created'])\n",
    "# posts\n",
    "\n",
    "submission = reddit.submission(id=\"qttjxb\")\n",
    "for top_level_comment in submission.comments:\n",
    "    print(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "\n",
    "PUSHSHIFT_REDDIT_URL = \"http://api.pushshift.io/reddit\"\n",
    "\n",
    "def fetchObjects(**kwargs):\n",
    "    # Default paramaters for API query\n",
    "    params = {\n",
    "        \"sort_type\":\"score\",\n",
    "        \"sort\":\"desc\",\n",
    "        \"size\":1000\n",
    "        }\n",
    "\n",
    "    # Add additional paramters based on function arguments\n",
    "    for key,value in kwargs.items():\n",
    "        params[key] = value\n",
    "\n",
    "    # Print API query paramaters\n",
    "    print(params)\n",
    "\n",
    "    # Set the type variable based on function input\n",
    "    # The type can be \"comment\" or \"submission\", default is \"comment\"\n",
    "    type = \"comment\"\n",
    "    if 'type' in kwargs and kwargs['type'].lower() == \"submission\":\n",
    "        type = \"submission\"\n",
    "    \n",
    "    # Perform an API request\n",
    "    r = requests.get(PUSHSHIFT_REDDIT_URL + \"/\" + type + \"/search/\", params=params, timeout=30)\n",
    "\n",
    "    # Check the status code, if successful, process the data\n",
    "    if r.status_code == 200:\n",
    "        response = json.loads(r.text)\n",
    "        data = response['data']\n",
    "        sorted_data_by_id = sorted(data, key=lambda x: int(x['id'],36))\n",
    "        return sorted_data_by_id\n",
    "\n",
    "def extract_reddit_data(**kwargs):\n",
    "    # Speficify the start timestamp\n",
    "    max_created_utc = 1105318800  # 01/01/2013 @ 12:00am (UTC)\n",
    "    max_id = 0\n",
    "\n",
    "    # Open a file for JSON output\n",
    "    file = open(\"submissions.json\",\"w\")\n",
    "\n",
    "    # While loop for recursive function\n",
    "    while 1:\n",
    "        nothing_processed = True\n",
    "        # Call the recursive function\n",
    "        objects = fetchObjects(**kwargs,after=max_created_utc)\n",
    "        \n",
    "        # Loop the returned data, ordered by date\n",
    "        for object in objects:\n",
    "            id = int(object['id'],36)\n",
    "            if id > max_id:\n",
    "                nothing_processed = False\n",
    "                created_utc = object['created_utc']\n",
    "                max_id = id\n",
    "                if created_utc > max_created_utc: max_created_utc = created_utc\n",
    "                # Output JSON data to the opened file\n",
    "                print(json.dumps(object,sort_keys=True,ensure_ascii=True),file=file)\n",
    "        \n",
    "        # Exit if nothing happened\n",
    "        if nothing_processed: return\n",
    "        max_created_utc -= 1\n",
    "\n",
    "        # Sleep a little before the next recursive function call\n",
    "        time.sleep(.5)\n",
    "\n",
    "# Start program by calling function with:\n",
    "# 1) Subreddit specified\n",
    "# 2) The type of data required (comment or submission)\n",
    "extract_reddit_data(subreddit=\"gamingsuggestions\",type=\"submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_json(\"submissions.json\", lines=True)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "\n",
    "PUSHSHIFT_REDDIT_URL = \"http://api.pushshift.io/reddit\"\n",
    "\n",
    "def fetchObjects(**kwargs):\n",
    "    # Default params values\n",
    "    params = {\"sort_type\":\"score\",\"sort\":\"asc\",\"size\":1000}\n",
    "    for key,value in kwargs.items():\n",
    "        params[key] = value\n",
    "    print(params)\n",
    "    type = \"comment\"\n",
    "    if 'type' in kwargs and kwargs['type'].lower() == \"submission\":\n",
    "        type = \"submission\"\n",
    "    r = requests.get(PUSHSHIFT_REDDIT_URL + \"/\" + type + \"/search/\",params=params)\n",
    "    if r.status_code == 200:\n",
    "        response = json.loads(r.text)\n",
    "        data = response['data']\n",
    "        sorted_data_by__id = sorted(data, key=lambda x: int(x['id'],36))\n",
    "        return sorted_data_by__id\n",
    "\n",
    "def process(**kwargs):\n",
    "    max_created_utc = 1611784421\n",
    "    max_id = 0\n",
    "    file = open(\"Nsubmissions.json\",\"w\")\n",
    "    while 1:\n",
    "        nothing_processed = True\n",
    "        objects = fetchObjects(**kwargs,after=max_created_utc)\n",
    "        for object in objects:\n",
    "            id = int(object['id'],36)\n",
    "            if id > max_id:\n",
    "                nothing_processed = False\n",
    "                created_utc = object['created_utc']\n",
    "                max_id = id\n",
    "                if created_utc > max_created_utc: max_created_utc = created_utc\n",
    "                print(json.dumps(object,sort_keys=True,ensure_ascii=True),file=file)\n",
    "        if nothing_processed: return\n",
    "        max_created_utc -= 1\n",
    "        time.sleep(.5)\n",
    "\n",
    "process(subreddit=\"gamingsuggestions\",type=\"submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= pd.read_json(\"submissions.json\", lines=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
